{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8873df",
   "metadata": {},
   "source": [
    "# new 10.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e04cc0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.55.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers sentencepiece torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f24ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M.S.I\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# atau untuk BERT2GPT:\n",
    "# from transformers import BertTokenizer, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6524743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pastikan sentencepiece sudah ada ---\n",
    "try:\n",
    "    import sentencepiece\n",
    "except ImportError:\n",
    "    os.system(\"pip install sentencepiece\")\n",
    "    import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55f5a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M.S.I\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\M.S.I\\.cache\\huggingface\\hub\\models--panggi--t5-base-indonesian-summarization-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# --- Load model & tokenizer ---\n",
    "model_name = \"panggi/t5-base-indonesian-summarization-cased\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Jika pakai BERT2GPT:\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"cahya/bert2gpt-indonesian-summarization\")\n",
    "# tokenizer.bos_token = tokenizer.cls_token\n",
    "# tokenizer.eos_token = tokenizer.sep_token\n",
    "# model = EncoderDecoderModel.from_pretrained(\"cahya/bert2gpt-indonesian-summarization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ec9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Baca teks dari file ---\n",
    "input_file = \"source-chatbot.txt\"\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"File {input_file} tidak ditemukan!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3bd94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ce7d3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info sebelum ringkas\n",
    "len_chars_before = len(text)\n",
    "len_words_before = len(text.split())\n",
    "min_words_target = int(len_words_before * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1ecf7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi meringkas satu chunk dengan minimal kata\n",
    "def summarize_chunk(chunk_text, min_words=1000, max_input_tokens=512):\n",
    "    # Estimasi max_output_tokens sesuai target kata\n",
    "    max_output_tokens = int(min_words * 1.3)  # tambahan buffer\n",
    "    input_ids = tokenizer.encode(chunk_text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_output_tokens,\n",
    "        min_length=min_words,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pecah teks jadi beberapa bagian (per ~2000 kata)\n",
    "words = text.split()\n",
    "chunk_size = 2000\n",
    "chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86977535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung minimal kata per chunk\n",
    "min_words_per_chunk = int(chunk_size * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97566c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merangkum bagian 1/11...\n",
      "Merangkum bagian 2/11...\n",
      "Merangkum bagian 3/11...\n",
      "Merangkum bagian 4/11...\n",
      "Merangkum bagian 5/11...\n",
      "Merangkum bagian 6/11...\n",
      "Merangkum bagian 7/11...\n",
      "Merangkum bagian 8/11...\n",
      "Merangkum bagian 9/11...\n",
      "Merangkum bagian 10/11...\n",
      "Merangkum bagian 11/11...\n"
     ]
    }
   ],
   "source": [
    "# Ringkas tiap chunk & simpan\n",
    "all_summaries = []\n",
    "with open(\"summary_parts.txt\", \"w\", encoding=\"utf-8\") as f_parts:\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        print(f\"Merangkum bagian {i}/{len(chunks)}...\")\n",
    "        summary_part = summarize_chunk(chunk, min_words=min_words_per_chunk)\n",
    "        all_summaries.append(summary_part)\n",
    "        f_parts.write(f\"--- Ringkasan Bagian {i} ---\\n{summary_part}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ac7e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua ringkasan chunk â†’ hasil akhir\n",
    "final_summary = \"\\n\\n\".join(all_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7092eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info sesudah ringkas\n",
    "len_chars_after = len(final_summary)\n",
    "len_words_after = len(final_summary.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18a97e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Perbandingan Panjang Teks ===\n",
      "Sebelum: 149304 karakter, 20337 kata\n",
      "Sesudah: 35347 karakter, 5059 kata\n",
      "Target minimal: 10168 kata\n"
     ]
    }
   ],
   "source": [
    "# Tampilkan info\n",
    "print(\"\\n=== Perbandingan Panjang Teks ===\")\n",
    "print(f\"Sebelum: {len_chars_before} karakter, {len_words_before} kata\")\n",
    "print(f\"Sesudah: {len_chars_after} karakter, {len_words_after} kata\")\n",
    "print(f\"Target minimal: {min_words_target} kata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ec35de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ringkasan berhasil disimpan!\n"
     ]
    }
   ],
   "source": [
    "# Simpan hasil\n",
    "with open(\"source-chatbot-short.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_summary)\n",
    "\n",
    "print(f\"\\nRingkasan berhasil disimpan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c94000",
   "metadata": {},
   "source": [
    "# Google-Generative 11.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f19c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c22f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text_file(file_path, api_key, model=\"gemini-2.5-flash\", max_tokens=200):\n",
    "    # Inisialisasi klien dengan API key\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    \n",
    "    # Baca isi file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Kirim permintaan generative AI untuk merangkum\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=text,\n",
    "        config=types.GenerateContentConfig(max_output_tokens=max_tokens)\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0559991",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyD1T_DQ4VTk8jKdtkcrHhZjTaXKT-0222s\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97947a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"source-chatbot.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c36131b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung jumlah kata awal\n",
    "original_word_count = len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d6f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt untuk membersihkan pengulangan, bukan merangkum pendek\n",
    "prompt = f\"\"\"\n",
    "Bersihkan teks berikut dari kalimat atau ide yang berulang, \n",
    "gabungkan kalimat yang mirip, dan pertahankan semua informasi penting, detail, dan data.\n",
    "Jangan hapus fakta penting. Jangan ringkas berlebihan.\n",
    "Teks:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d42ba8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7ab1b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil hasil ringkasan\n",
    "summary = response.text.strip()\n",
    "\n",
    "# Hitung jumlah kata ringkasan\n",
    "summary_word_count = len(summary.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abc70b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah kata sebelum: 20337\n",
      "Jumlah kata sesudah: 2623\n"
     ]
    }
   ],
   "source": [
    "# Tampilkan info jumlah kata\n",
    "print(f\"Jumlah kata sebelum: {original_word_count}\")\n",
    "print(f\"Jumlah kata sesudah: {summary_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c49226ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil ringkasan berhasil disimpan ke: source-chatbot-short.txt\n"
     ]
    }
   ],
   "source": [
    "# Simpan hasil ringkasan ke file\n",
    "output_file = \"source-chatbot-short.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\nHasil ringkasan berhasil disimpan ke: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba64c5c",
   "metadata": {},
   "source": [
    "# HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7956a826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.13.0-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 14.5 MB/s  0:00:00\n",
      "Installing collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b8a6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import google.generativeai as genai\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28c970d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 1: Deduplication Kasar ====\n",
    "def deduplicate_text(text, similarity_threshold=90):\n",
    "    sentences = sent_tokenize(text)\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "        if not any(fuzz.ratio(clean_sentence, s) > similarity_threshold for s in unique_sentences):\n",
    "            unique_sentences.append(clean_sentence)\n",
    "    return \" \".join(unique_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a45a3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=5000):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = \" \".join(words[i:i+max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "648d34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== STEP 2: Rapikan dengan Gemini ====\n",
    "# ===== Refine dengan Gemini =====\n",
    "def refine_with_gemini(text, api_key, model_name=\"gemini-1.5-flash\"):\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Rapikan teks berikut tanpa menghapus informasi penting.\n",
    "    Tugas Anda:\n",
    "    1. Gabungkan kalimat atau ide yang sama/serupa.\n",
    "    2. Hilangkan pengulangan kata/kalimat yang berlebihan.\n",
    "    3. Pertahankan semua fakta, angka, data, dan detail penting.\n",
    "    4. Jangan memendekkan teks secara signifikan.\n",
    "    5. Jaga agar hasil akhir tetap panjang, kaya informasi, dan enak dibaca.\n",
    "\n",
    "    Berikut teksnya:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9f5cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\M.S.I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\M.S.I\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6cd7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyD1T_DQ4VTk8jKdtkcrHhZjTaXKT-0222s\"\n",
    "input_file = \"source-chatbot.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a8cb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca file asli\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    original_text = f.read()\n",
    "\n",
    "# Hitung kata awal\n",
    "original_count = len(original_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a734eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Deduplication NLP\n",
    "dedup_text = deduplicate_text(original_text)\n",
    "dedup_count = len(dedup_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8451e52",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 2: Chunking\n",
    "chunks = chunk_text(dedup_text, max_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87f1275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memproses chunk 1/5...\n",
      "Memproses chunk 2/5...\n",
      "Memproses chunk 3/5...\n",
      "Memproses chunk 4/5...\n",
      "Memproses chunk 5/5...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Refine tiap chunk\n",
    "refined_chunks = []\n",
    "for idx, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Memproses chunk {idx}/{len(chunks)}...\")\n",
    "    refined_chunk = refine_with_gemini(chunk, API_KEY)\n",
    "    refined_chunks.append(refined_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "029c3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Gabungkan hasil\n",
    "final_text = \"\\n\\n\".join(refined_chunks)\n",
    "final_count = len(final_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "741d2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil akhir\n",
    "output_file = \"source-chatbot-short-hybrid.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67b46f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INFO HASIL ===\n",
      "Jumlah kata awal       : 20337\n",
      "Setelah dedup NLP      : 20148\n",
      "Setelah refine Gemini  : 5331\n",
      "Hasil akhir disimpan di: source-chatbot-short-hybrid.txt\n"
     ]
    }
   ],
   "source": [
    "# Info hasil\n",
    "print(\"\\n=== INFO HASIL ===\")\n",
    "print(f\"Jumlah kata awal       : {original_count}\")\n",
    "print(f\"Setelah dedup NLP      : {dedup_count}\")\n",
    "print(f\"Setelah refine Gemini  : {final_count}\")\n",
    "print(f\"Hasil akhir disimpan di: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fb370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== MAIN PROGRAM ====\n",
    "if __name__ == \"__main__\":\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    API_KEY = \"YOUR_API_KEY\"\n",
    "    input_file = \"file.txt\"\n",
    "\n",
    "    # Baca file asli\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        original_text = f.read()\n",
    "\n",
    "    # Hitung kata awal\n",
    "    original_count = len(original_text.split())\n",
    "\n",
    "    # Step 1: Deduplication NLP\n",
    "    dedup_text = deduplicate_text(original_text)\n",
    "    dedup_count = len(dedup_text.split())\n",
    "\n",
    "    # Step 2: Refine dengan Gemini\n",
    "    refined_text = refine_with_gemini(dedup_text, API_KEY)\n",
    "    refined_count = len(refined_text.split())\n",
    "\n",
    "    # Simpan hasil akhir\n",
    "    output_file = \"file_bersih.txt\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(refined_text)\n",
    "\n",
    "    # Info hasil\n",
    "    print(f\"Jumlah kata awal       : {original_count}\")\n",
    "    print(f\"Setelah dedup NLP      : {dedup_count}\")\n",
    "    print(f\"Setelah refine Gemini  : {refined_count}\")\n",
    "    print(f\"Hasil akhir disimpan di: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
